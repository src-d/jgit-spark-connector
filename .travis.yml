dist: trusty
sudo: required

language: scala

scala:
   - 2.11.11
jdk:
  - jdk8
  - openjdk8

cache:
  directories:
    - $HOME/.sbt
    - $HOME/.ivy2
    - .spark-dist

services:
  - docker

matrix:
  fast_finish: true
  include:
    - language: python
      python: 2.7

      install:
        - ./local-install-enry-java.sh
        - ./sbt assembly
        - mkdir -p python/jars
        - cp target/**/spark-api-uber.jar python/jars
        - cd python
        - cp ../local-install-enry-java.sh .
        - pip install -r requirements.txt

      script:
        - python -m unittest discover -v

      after_success:
        - echo "" # noop to avoid executing the one from the main build

    - language: python
      python: 3.6

      install:
        - ./local-install-enry-java.sh
        - ./sbt assembly
        - mkdir -p python/jars
        - cp target/**/spark-api-uber.jar python/jars
        - cd python
        - cp ../local-install-enry-java.sh .
        - pip install -r requirements.txt

      script:
        - python -m unittest discover -v

      after_success:
        - echo "" # noop to avoid executing the one from the main build

      before_deploy:
        - echo "$TRAVIS_TAG" | cut -c 2- > version.txt

      deploy:
        provider: pypi
        user: $PYPI_USERNAME
        password: $PYPI_PASSWORD
        skip_cleanup: true
        on:
          tags: true

before_script:
  - ./_tools/local-install-enry-java.sh
  - docker run --privileged -d -p 9432:9432 --name bblfsh bblfsh/server

script:
  - ./sbt ++$TRAVIS_SCALA_VERSION jacoco:cover || travis_terminate 1
  - ./sbt package
  - ./_tools/getApacheSpark.sh "2.2.0" "2.7"
  - ./spark-shell -i src/test/resources/SparkShellScript.scala

after_success:
  - bash <(curl -s https://codecov.io/bash)